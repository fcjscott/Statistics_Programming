{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJj2C9z4-3UY"
   },
   "source": [
    "# STAT202A HW5 --- Convolutional Networks\n",
    "So far we have worked with fully-connected networks, using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, but in practice all state-of-the-art results use convolutional networks instead.\n",
    "\n",
    "In this homework, your are required to implement convolution layer, forward and backward. All other part of the code are given. You will then use these layers to train a convolutional network on the CIFAR-10 dataset.\n",
    "\n",
    "Please following these steps,\n",
    "\n",
    "- STEP 1: Implement two function in second block, CNN forward and backward.\n",
    "- STEP 2: Read the code I provided, especially code in this pythonbook, the definition of CNN network in second block class. Add comments to codes in second blocks. (You can also read code provided in zip, but you do not need to make comment)\n",
    "- STEP 3: Run each block one by one see every thing works well.\n",
    "- STEP 4: Try to turn the learning rate and other setting to make final cifar learning well. Notice the cifar training use fast version CNN so this is not affected by your implementation. i.e. even you you fail to implement CNN layer, you can still it play it.\n",
    "- STEP 5: Doing more extra play at the end of this pythonbook. e.g. : Try to virtualize more filter / try to plot an accuracy according to different setting / calculate the accuracy by each class ... It is extra and optional.\n",
    "- STEP 6: Press Ctrl + P (or Commend + P) to print this page to pdf. Then download this ipynb files. \n",
    "- STEP 7: Submit the pdf and ipynb files only to ccle. (Two files, pdf and ipynb, no other filetype accepted)\n",
    "\n",
    "In order to useit in  google Colab, **remember to change Runtime -> change runtime type -> python version from python 3 to python 2**. Then run the first block, select the zip files I provided to upload and this block of code will automaticlly unzip it. Then, it will download cifar files and makefiles. \n",
    "\n",
    "If any problem caused later and crack the runtime. Remember to reset the runtime by Runtime -> Reset all runtimes and rerun the first block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LqQffH-R_0VL"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "!unzip HW5_code\n",
    "!pip install Cython==0.21\n",
    "!python HW5_code/setup.py build_ext --inplace\n",
    "!mv im2col_cython.c HW5_code/im2col_cython.c\n",
    "!mv im2col_cython.so HW5_code/im2col_cython.so\n",
    "!mv im2col_cython.pyx HW5_code/im2col_cython.pyx\n",
    "!wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "!tar -xzvf cifar-10-python.tar.gz\n",
    "!rm cifar-10-python.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6C9ZWqH0CE5T"
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from HW5_code.data_utils import get_CIFAR10_data\n",
    "from HW5_code.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from HW5_code.layers import *\n",
    "from HW5_code.fast_layers import *\n",
    "from HW5_code.solver import Solver\n",
    "from HW5_code.layer_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "class ThreeLayerConvNet(object):\n",
    "    \"\"\"\n",
    "    A three-layer convolutional network with the following architecture:\n",
    "    conv - relu - 2x2 max pool - fc - relu - fc - softmax\n",
    "    The network operates on minibatches of data that have shape (N, C, H, W)\n",
    "    consisting of N images, each with height H and width W and with C input\n",
    "    channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_filters=32, filter_size=7,\n",
    "                 hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0,\n",
    "                 dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Initialize a new network.\n",
    "        Inputs:\n",
    "        - input_dim: Tuple (C, H, W) giving size of input data\n",
    "        - num_filters: Number of filters to use in the convolutional layer\n",
    "        - filter_size: Size of filters to use in the convolutional layer\n",
    "        - hidden_dim: Number of units to use in the fully-connected hidden layer\n",
    "        - num_classes: Number of scores to produce from the final fc layer.\n",
    "        - weight_scale: Scalar giving standard deviation for random initialization\n",
    "          of weights.\n",
    "        - reg: Scalar giving L2 regularization strength\n",
    "        - dtype: numpy datatype to use for computation.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.reg = reg\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        C, H, W = input_dim\n",
    "        self.params['W1'] = np.random.normal(0, weight_scale, [num_filters, 3, filter_size, filter_size])\n",
    "        self.params['b1'] = np.zeros([num_filters])\n",
    "        self.params['W2'] = np.random.normal(0, weight_scale, [np.int(H/2)*np.int(H/2)*num_filters, hidden_dim])\n",
    "        self.params['b2'] = np.zeros([hidden_dim])\n",
    "        self.params['W3'] = np.random.normal(0, weight_scale, [hidden_dim, num_classes])\n",
    "        self.params['b3'] = np.zeros([num_classes])\n",
    "\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Evaluate loss and gradient for the three-layer convolutional network.\n",
    "        Input / output: Same API as TwoLayerNet in fc_net.py.\n",
    "        \"\"\"\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        W3, b3 = self.params['W3'], self.params['b3']\n",
    "\n",
    "        # pass conv_param to the forward pass for the convolutional layer\n",
    "        filter_size = W1.shape[2]\n",
    "        conv_param = {'stride': 1, 'pad': (filter_size - 1) // 2}\n",
    "\n",
    "        # pass pool_param to the forward pass for the max-pooling layer\n",
    "        pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "        scores = None\n",
    "        \n",
    "        layer1_out, combined_cache = conv_relu_pool_forward(X, W1, b1,  conv_param, pool_param)\n",
    "        fc1_out, fc1_cache = fc_forward(layer1_out, W2, b2)\n",
    "        relu2_out, relu2_cache = relu_forward(fc1_out)\n",
    "        fc2_out, fc2_cache = fc_forward(relu2_out, W3, b3)\n",
    "\n",
    "        scores = np.copy(fc2_out)\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0, {}\n",
    "        \n",
    "        loss, dsoft = softmax_loss(scores, y)\n",
    "        loss += self.reg*0.5*(np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\n",
    "\n",
    "        dx3, dw3, db3 = fc_backward(dsoft, fc2_cache)\n",
    "        drelu2 = relu_backward(dx3, relu2_cache)\n",
    "        dx2, dw2, db2 = fc_backward(drelu2, fc1_cache)\n",
    "        dx1, dw1, db1 = conv_relu_pool_backward(dx2, combined_cache)\n",
    "\n",
    "        grads['W3'], grads['b3'] = dw3 + self.reg*W3, db3\n",
    "        grads['W2'], grads['b2'] = dw2 + self.reg*W2, db2\n",
    "        grads['W1'], grads['b1'] = dw1 + self.reg*W1, db1\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "\n",
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a convolutional layer.\n",
    "\n",
    "    The input consists of N data points, each with C channels, height H and\n",
    "    width W. We convolve each input with F different filters, where each filter\n",
    "    spans all C channels and has height HH and width HH.\n",
    "\n",
    "    Input:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - w: Filter weights of shape (F, C, HH, WW)\n",
    "    - b: Biases, of shape (F,)\n",
    "    - conv_param: A dictionary with the following keys:\n",
    "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "        horizontal and vertical directions.\n",
    "      - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "      H' = 1 + (H + 2 * pad - HH) / stride\n",
    "      W' = 1 + (W + 2 * pad - WW) / stride\n",
    "    - cache: (x, w, b, conv_param)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "\n",
    "    pad = conv_param.get('pad')\n",
    "    stride = conv_param.get('stride')\n",
    "    N, C, H, W = x.shape\n",
    "    F, C, HH, WW = w.shape\n",
    "\n",
    "    padded_x = # Define it\n",
    "    out_H = # Define it\n",
    "    out_W = # Define it\n",
    "    out = # Define it\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the convolutional forward pass.                         #\n",
    "    # Hint: you can use the function np.pad for padding.                      #\n",
    "    ###########################################################################\n",
    "                        \n",
    "    cache = (x, w, b, conv_param)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def conv_backward_naive(dout, cache):\n",
    "    \"\"\"\n",
    "    A naive implementation of the backward pass for a convolutional layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives.\n",
    "    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x\n",
    "    - dw: Gradient with respect to w\n",
    "    - db: Gradient with respect to b\n",
    "    \"\"\"\n",
    "    dx, dw, db = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the convolutional backward pass.                        #\n",
    "    ###########################################################################\n",
    "    \n",
    "    x, w, b, conv_param = cache\n",
    "    stride = conv_param.get('stride')\n",
    "    pad = conv_param.get('pad')\n",
    "    padded_x = (np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), 'constant'))\n",
    "\n",
    "    N, C, H, W = x.shape\n",
    "    F, C, HH, WW = w.shape\n",
    "    N, F, H_out, W_out = dout.shape\n",
    "\n",
    "    dx_temp = np.zeros_like(padded_x)\n",
    "    dw = np.zeros_like(w)\n",
    "    db = np.zeros_like(b)\n",
    "\n",
    "    # Calculate dB.\n",
    "    \n",
    "    # Calculate dw.\n",
    "    \n",
    "    # Calculate dx.\n",
    "    \n",
    "    return dx, dw, db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRyiGqp9-3Uh"
   },
   "outputs": [],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WO2r9nOa-3Uq"
   },
   "source": [
    "# Convolution: Naive forward pass\n",
    "The core of a convolutional network is the convolution operation. In the file `stats232a/layers.py`, implement the forward pass for the convolution layer in the function `conv_forward_naive`. \n",
    "\n",
    "You don't have to worry too much about efficiency at this point; just write the code in whatever way you find most clear.\n",
    "\n",
    "You can test your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDXKQokg-3Ut"
   },
   "outputs": [],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
    "correct_out = np.array([[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]])\n",
    "\n",
    "# Compare your output to ours; difference should be around 2e-8\n",
    "print('Testing conv_forward_naive')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VuA0qnU2-3U5"
   },
   "source": [
    "# Convolution: Naive backward pass\n",
    "Implement the backward pass for the convolution operation in the function `conv_backward_naive` in the file `stats232a/layers.py`. Again, you don't need to worry too much about computational efficiency.\n",
    "\n",
    "When you are done, run the following to check your backward pass with a numeric gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzndzpbP-3U6"
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = conv_forward_naive(x, w, b, conv_param)\n",
    "dx, dw, db = conv_backward_naive(dout, cache)\n",
    "\n",
    "# Your errors should be around 1e-8'\n",
    "print('Testing conv_backward_naive function')\n",
    "print('dx error: ', rel_error(dx, dx_num))\n",
    "print('dw error: ', rel_error(dw, dw_num))\n",
    "print('db error: ', rel_error(db, db_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sI8YpUhP-3W5"
   },
   "source": [
    "# Fast layers\n",
    "Making convolution and pooling layers fast can be challenging. To spare you the pain, we've provided fast implementations of the forward and backward passes for convolution and pooling layers in the file `stats232a/fast_layers.py`.\n",
    "\n",
    "The fast convolution implementation depends on a Cython extension; to compile it you need to run the following from the `stats232a` directory:\n",
    "\n",
    "```bash\n",
    "python setup.py build_ext --inplace\n",
    "```\n",
    "\n",
    "The API for the fast versions of the convolution and pooling layers is exactly the same as the naive versions that you implemented above: the forward pass receives data, weights, and parameters and produces outputs and a cache object; the backward pass recieves upstream derivatives and the cache object and produces gradients with respect to the data and weights.\n",
    "\n",
    "**NOTE:** The fast implementation for pooling will only perform optimally if the pooling regions are non-overlapping and tile the input. If these conditions are not met then the fast pooling implementation will not be much faster than the naive implementation.\n",
    "\n",
    "You can compare the performance of the naive and fast versions of these layers by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdXYpdis-3W8"
   },
   "outputs": [],
   "source": [
    "from HW5_code.fast_layers import conv_forward_fast, conv_backward_fast\n",
    "from time import time\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(100, 3, 31, 31)\n",
    "w = np.random.randn(25, 3, 3, 3)\n",
    "b = np.random.randn(25,)\n",
    "dout = np.random.randn(100, 25, 16, 16)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = conv_forward_naive(x, w, b, conv_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = conv_forward_fast(x, w, b, conv_param)\n",
    "t2 = time()\n",
    "\n",
    "print('Testing conv_forward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('Fast: %fs' % (t2 - t1))\n",
    "print('Speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('Difference: ', rel_error(out_naive, out_fast))\n",
    "\n",
    "t0 = time()\n",
    "dx_naive, dw_naive, db_naive = conv_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast, dw_fast, db_fast = conv_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print('\\nTesting conv_backward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('Fast: %fs' % (t2 - t1))\n",
    "print('Speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('dx difference: ', rel_error(dx_naive, dx_fast))\n",
    "print('dw difference: ', rel_error(dw_naive, dw_fast))\n",
    "print('db difference: ', rel_error(db_naive, db_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRRLDOYH-3XA"
   },
   "outputs": [],
   "source": [
    "from HW5_code.fast_layers import max_pool_forward_fast, max_pool_backward_fast\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(100, 3, 32, 32)\n",
    "dout = np.random.randn(100, 3, 16, 16)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = max_pool_forward_naive(x, pool_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = max_pool_forward_fast(x, pool_param)\n",
    "t2 = time()\n",
    "\n",
    "print('Testing pool_forward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('fast: %fs' % (t2 - t1))\n",
    "print('speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('difference: ', rel_error(out_naive, out_fast))\n",
    "\n",
    "t0 = time()\n",
    "dx_naive = max_pool_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast = max_pool_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print('\\nTesting pool_backward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('dx difference: ', rel_error(dx_naive, dx_fast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILANeX-z-3Xv"
   },
   "source": [
    "# Three-layer ConvNet\n",
    "Now that you have implemented all the necessary layers, we can put them together into a simple convolutional network.\n",
    "\n",
    "Open the file `stats232a/classifiers/cnn.py` and complete the implementation of the `ThreeLayerConvNet` class. Run the following cells to help you debug:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itHSETJ2-3Xw"
   },
   "source": [
    "## Sanity check loss\n",
    "After you build a new network, one of the first things you should do is sanity check the loss. When we use the softmax loss, we expect the loss for random weights (and no regularization) to be about `log(C)` for `C` classes. When we add regularization this should go up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kv0O3yoA-3Xy"
   },
   "outputs": [],
   "source": [
    "model = ThreeLayerConvNet()\n",
    "\n",
    "N = 50\n",
    "X = np.random.randn(N, 3, 32, 32)\n",
    "y = np.random.randint(10, size=N)\n",
    "\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (no regularization): ', loss)\n",
    "\n",
    "model.reg = 0.5\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (with regularization): ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1_xvb7b-3YG"
   },
   "source": [
    "## Gradient check\n",
    "After the loss looks reasonable, use numeric gradient checking to make sure that your backward pass is correct. When you use numeric gradient checking you should use a small amount of artifical data and a small number of neurons at each layer. Note: correct implementations may still have relative errors up to 1e-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBFCiUq1-3YJ"
   },
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "input_dim = (3, 16, 16)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "np.random.seed(231)\n",
    "X = np.random.randn(num_inputs, *input_dim)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = ThreeLayerConvNet(num_filters=3, filter_size=3,\n",
    "                          input_dim=input_dim, hidden_dim=7,\n",
    "                          dtype=np.float64)\n",
    "loss, grads = model.loss(X, y)\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3KVdehwr-3YS"
   },
   "source": [
    "## Overfit small data\n",
    "A nice trick is to train your model with just a few training samples. You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrToXqrv-3YT"
   },
   "outputs": [],
   "source": [
    "np.random.seed(6666666)\n",
    "\n",
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = ThreeLayerConvNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=15, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ecoSFk2y-3YZ"
   },
   "source": [
    "Plotting the loss, training accuracy, and validation accuracy should show clear overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8WyU2tJ-3Yb"
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RxC48Xkv-3Yg"
   },
   "source": [
    "## Train the net\n",
    "By training the three-layer convolutional network for one epoch, you should achieve greater than 40% accuracy on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VAhfjmNS-3Yi",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = ThreeLayerConvNet(reg=0.001)\n",
    "\n",
    "# Try to change update_rule : 'sgd' / 'sgd_momentum' / 'rmsprop' / 'adam'\n",
    "# Then try learning rate and reg.\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=1, batch_size=200,\n",
    "                update_rule='???',\n",
    "                optim_config={\n",
    "                  'learning_rate': ???,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "21RGsBxx-3Y3"
   },
   "source": [
    "## Visualize Filters\n",
    "You can visualize the first-layer convolutional filters from the trained network by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ldg8_urL-3Y5"
   },
   "outputs": [],
   "source": [
    "from stats232a.vis_utils import visualize_grid\n",
    "\n",
    "grid = visualize_grid(model.params['W1'].transpose(0, 2, 3, 1))\n",
    "plt.imshow(grid.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJayvx9cwJGm"
   },
   "source": [
    "##Extra Credit\n",
    "\n",
    "Try to more interesting observation as you wish! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_FdmobjVwIqc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ne3oADJFyyQR"
   },
   "source": [
    "Copyright: This tamplate is original from Stanford cs231n opensource online course. Modification is made by Ruiqi Gao and Yifei Xu. No distribution is permitted."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW5_CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
