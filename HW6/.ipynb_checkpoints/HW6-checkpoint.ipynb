{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "## Stat 202A - Homework 6 SVM & Adaboost\n",
    "## Author: Chaojie Feng\n",
    "## Date : Nov.25, 2018\n",
    "## Description: This script implements a support vector machine, an adaboost classifier\n",
    "#########################################################\n",
    "\n",
    "#############################################################\n",
    "## INSTRUCTIONS: Please fill in the missing lines of code\n",
    "## only where specified. Do not change function names,\n",
    "## function inputs or outputs. You can add examples at the\n",
    "## end of the script (in the \"Optional examples\" section) to\n",
    "## double-check your work, but MAKE SURE TO COMMENT OUT ALL\n",
    "## OF YOUR EXAMPLES BEFORE SUBMITTING.\n",
    "##\n",
    "## Very important: Do not use the function \"os.chdir\" anywhere\n",
    "## in your code. If you do, I will be unable to grade your\n",
    "## work since Python will attempt to change my working directory\n",
    "## to one that does not exist.\n",
    "#############################################################\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.datasets as ds\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def prepare_data(valid_digits=np.array((6, 5))):\n",
    "    ## valid_digits is a vector containing the digits\n",
    "    ## we wish to classify.\n",
    "    ## Do not change anything inside of this function\n",
    "    if len(valid_digits) != 2:\n",
    "        raise Exception(\n",
    "            \"Error: you must specify exactly 2 digits for classification!\")\n",
    "\n",
    "    data = ds.load_digits()\n",
    "    labels = data['target']\n",
    "    features = data['data']\n",
    "    X = features[(labels == valid_digits[0]) | (labels == valid_digits[1]), :]\n",
    "    Y = labels[(labels == valid_digits[0]) | (labels == valid_digits[1]), ]\n",
    "    X = X / np.repeat(np.max(X, axis=1), 64).reshape(X.shape[0], -1)\n",
    "\n",
    "    Y[Y == valid_digits[0]] = 0\n",
    "    Y[Y == valid_digits[1]] = 1\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=0.25, random_state=10)\n",
    "    Y_train = Y_train.reshape((len(Y_train), 1))\n",
    "    Y_test = Y_test.reshape((len(Y_test), 1))\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "####################################################\n",
    "##           1: Support vector machine            ##\n",
    "####################################################\n",
    "\n",
    "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ##\n",
    "## Train an SVM to classify the digits data ##\n",
    "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ##\n",
    "\n",
    "def my_SVM(X_train, Y_train, X_test, Y_test, lamb=0.01, num_iterations=200, learning_rate=0.1):\n",
    "    ## X_train: Training set of features\n",
    "    ## Y_train: Training set of labels corresponding to X_train\n",
    "    ## X_test: Testing set of features\n",
    "    ## Y_test: Testing set of labels correspdonding to X_test\n",
    "    ## lamb: Regularization parameter\n",
    "    ## num_iterations: Number of iterations.\n",
    "    ## learning_rate: Learning rate.\n",
    "\n",
    "    ## Function should learn the parameters of an SVM.\n",
    "    ## Intercept term is needed.\n",
    "\n",
    "    #######################\n",
    "    ## FILL IN CODE HERE ##\n",
    "    #######################\n",
    "    n,p = X_train.shape\n",
    "\n",
    "    ntest,_ = X_test.shape\n",
    "    \n",
    "    X_train1 = np.hstack((np.ones((n,1)), X_train))\n",
    "    X_test1 = np.hstack((np.ones((ntest, 1)), X_test))\n",
    "    \n",
    "    \n",
    "    Y_train = 2*Y_train - 1\n",
    "    Y_test = 2*Y_test - 1\n",
    "    beta = np.random.rand((p+1),1)\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        s = X_train1.dot(beta)\n",
    "        de = s * Y_train < 1\n",
    "\n",
    "        dbeta = np.dot(np.ones((1, n)), de * Y_train * X_train1)/n\n",
    "\n",
    "        beta = beta + learning_rate*dbeta.T\n",
    "        \n",
    "        beta[1:] = beta[1:] - beta[1:]*lamb \n",
    "\n",
    "        y_train_pred = X_train1.dot(beta)\n",
    "        y_test_pred = X_test1.dot(beta)\n",
    "        \n",
    "        acc_train.append(np.mean(np.sign(y_train_pred) == Y_train))\n",
    "        acc_test.append(np.mean(np.sign(y_test_pred) == Y_test))\n",
    "    \n",
    "\n",
    "    ## Function should output 3 things:\n",
    "    ## 1. The learned parameters of the SVM, beta\n",
    "    ## 2. The accuracy over the training set, acc_train (a \"num_iterations\" dimensional vector).\n",
    "    ## 3. The accuracy over the testing set, acc_test (a \"num_iterations\" dimensional vector).\n",
    "\n",
    "    return beta, acc_train, acc_test\n",
    "\n",
    "######################################\n",
    "## Function 2: Adaboost ##\n",
    "######################################\n",
    "\n",
    "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ##\n",
    "## Use Adaboost to classify the digits data ##\n",
    "## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ##\n",
    "\n",
    "\n",
    "def my_Adaboost(X_train, Y_train, X_test, Y_test, num_iterations=200):\n",
    "    ## X_train: Training set of features\n",
    "    ## Y_train: Training set of labels corresponding to X_train\n",
    "    ## X_test: Testing set of features\n",
    "    ## Y_test: Testing set of labels correspdonding to X_test\n",
    "    ## num_iterations: Number of iterations.\n",
    "\n",
    "    ## Function should learn the parameters of an Adaboost classifier.\n",
    "    ## Intercept term is needed.\n",
    "\n",
    "    #######################\n",
    "    ## FILL IN CODE HERE ##\n",
    "    #######################\n",
    "    \n",
    "    \n",
    "    n, p = X_train.shape     \n",
    "    t = np.random.uniform()\n",
    "    X_train1 = 2 * (X_train > t) - 1\n",
    "    X_test1 = 2 * (X_test > t) - 1\n",
    "    \n",
    "    Y_train = 2 * Y_train - 1\n",
    "    Y_test = 2 * Y_test - 1\n",
    "    \n",
    "    \n",
    "    # initialize \n",
    "    beta = np.zeros((p, 1))\n",
    "    weight = np.ones((n , 1)) / n\n",
    "    weak_result = Y_train * X_train1 > 0 \n",
    "    acc_train = np.zeros(num_iterations)\n",
    "    acc_test = np.zeros(num_iterations)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        weight = weight / np.sum(weight)\n",
    "        weighted_result = weight * weak_result\n",
    "        weighted_accuracy = np.sum(weighted_result, axis = 0)\n",
    "        e = 1 - weighted_accuracy  \n",
    "        \n",
    "        # choose the classifier to minimize error\n",
    "        j = np.argmin(e)\n",
    "        \n",
    "\n",
    "        dbeta = 0.5 * np.log((1 - e[j]) / e[j])\n",
    "        beta[j] = beta[j] + dbeta\n",
    "        weight = weight * np.exp(- Y_train * X_train1[:, j].reshape((n, 1)) * dbeta)\n",
    "\n",
    "        # Accuracy\n",
    "        pred_train = np.dot(X_train1, beta)\n",
    "        acc_train[i] = np.mean(np.sign(pred_train) == Y_train)\n",
    "        pred_test = np.dot(X_test1, beta)\n",
    "        acc_test[i] = np.mean(np.sign(pred_test) == Y_test)  \n",
    "        \n",
    "\n",
    "    ## Function should output 3 things:\n",
    "    ## 1. The learned parameters of the adaboost classifier, beta\n",
    "    ## 2. The accuracy over the training set, acc_train (a \"num_iterations\" dimensional vector).\n",
    "    ## 3. The accuracy over the testing set, acc_test (a \"num_iterations\" dimensional vector).\n",
    "    return beta, acc_train, acc_test\n",
    "\n",
    "############################################################################\n",
    "## Testing your functions and visualize the results here##\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "#def testing_example():\n",
    "#\n",
    "#    ####################################################\n",
    "#    ## Optional examples (comment out your examples!) ##\n",
    "#    ####################################################\n",
    "#\n",
    "#    X_train, Y_train, X_test, Y_test = prepare_data()\n",
    "#\n",
    "#    beta, acc_train, acc_test = my_SVM(X_train, Y_train, X_test, Y_test)\n",
    "#\n",
    "#    ax = plt.plot(range(200), acc_train, range(200), acc_test)\n",
    "#    plt.xlabel('Number of iteration')\n",
    "#    plt.ylabel('Accuracy')\n",
    "#    plt.legend(('Training Accuracy', 'Testing Accuracy'))\n",
    "#    plt.show()\n",
    "#\n",
    "#    beta, acc_train, acc_test = my_Adaboost(X_train, Y_train, X_test, Y_test)\n",
    "#    plt.plot(range(200), acc_train, range(200), acc_test)\n",
    "#    plt.xlabel('Number of iteration')\n",
    "#    plt.ylabel('Accuracy')\n",
    "#    plt.legend(('Training Accuracy', 'Testing Accuracy'))\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
